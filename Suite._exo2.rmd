---
title: "Suite exo_2"
author: "HMMA201"
date: "05/02/2021"
output: html_document
---



##TD 1:
##La suite de l'exercice 2:
###Question 3:

Montrons que la variance de $\tilde{\beta}$ est strictement plus grande que la variance de $\hat{\beta}$, sauf dans le cas où les $x_{i}$ sont tous égaux. 



On s'intéresse toujours au modèle suivant:

$$
\forall{i \in (1,....,n)}; y_{i}=\beta_{1}+\beta_{2}*x_{i}+\epsilon_{i}; \\\\\\
 \epsilon_{i}= \text{les erreurs}. 
$$

De plus on note $\hat{\beta}$ l'estimateur OLS de $\beta$: 
$$ 
\hat{\beta}= \beta + \frac{\sum_{i=1}^{n}(x_{i}-\bar{x})\epsilon_{i}}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2} = \beta+ \frac{\sum_{i=1}^{n}x_{i}\epsilon_{i}}{\sum_{i=1}^{n}x_{i}^2}
$$
D'où: 
$$
Var(\hat{\beta})= Var(\beta)+Var(\frac{\sum_{i=1}^{n}x_{i}\epsilon_{i}}{\sum_{i=1}^{n}x_{i}^2})
$$

D'autre part; Comme les $\epsilon_{i}$ sont décoréllés.

$$
Var(\sum_{i=1}^{n}x_{i}\epsilon_{i})=cov(\sum_{i=1}^{n}x_{i}\epsilon_{i} , \sum_{j=1}^{n}x_{j}\epsilon_{j})
$$
Donc: 
$$ 
\begin{aligned}
 Var(\hat{\beta})
 & = \frac{Var(\sum_{i=1}^{n}x_{i}\epsilon_{i})}{(\sum_{i=1}^{n}x_{i}^2)^2} \\\\
 & = \frac{cov(\sum_{i=1}^{n}x_{i}\epsilon_{i} , \sum_{j=1}^{n}x_{j}\epsilon_{j})}{(\sum_{i=1}^{n}x_{i}^2)^2}\\\\
 & = \frac{\sum_{i=1}^{n}x_{i} \sum_{j=1}^{n}x_{j} cov(\epsilon_{i},\epsilon{j})}{(\sum_{i=1}^{n}x_{i}^2)^2}\\\\
 & = \frac{\sum_{i=1}^{n}x_{i}^2 cov(\epsilon_{i},\epsilon_{j})}{(\sum_{i=1}^{n}x_{i}^2)^2}\\\\
 & = \frac{cov(\epsilon_{i},\epsilon_{j})}{\sum_{i=1}^{n}x_{i}^2}
  = \frac{\sigma^{2}}{\sum_{i=1}^{n}x_{i}^2}
\end{aligned}
$$
On a trouvé donc que la variance de $\hat{\beta}$ est égale à $\frac{\sigma^{2}}{\sum_{i=1}^{n}x_{i}^2}$.

Ainsi on a; $\tilde{\beta}=\frac{\sum_{i=1}^{n}y_{i}}{\sum_{i=1}^{n}x_{i}}$, d'où:

$$
Var(\tilde{\beta})=\frac{\sum_{i=1}^{n}Var(\epsilon_{i})}{(\sum_{i=1}^{n}x_{i})^2}
$$
En effet: 
$$
Var(\sum_{i=1}^{n}y_{i})=Var(\sum_{i=1}^{n}\beta_{1}+\beta_{2} x_{i} + \epsilon_{i})=Var(\sum_{i=1}^{n}\epsilon_{i})=\sum_{i=1}^{n} Var(\epsilon_{i})
$$
Donc: 
$$
Var(\tilde{\beta})=\frac{nVar(\epsilon_{i})}{\sum_{i=1}^{n}x_{i}^2)^2}= \frac{n\sigma^{2}}{(\sum_{i=1}^{n}x_{i})^2}

$$
D'après les deux expressions de $Var(\tilde{\beta})$ et $Var(\hat{\beta})$ trouvées ci-dessus; 

$$
\begin{aligned}
Var(\tilde{\beta})= \frac{n\sigma^{2}}{(\sum_{i=1}^{n}x_{i})^2} \\\
Var(\hat{\beta})= \frac{\sigma^{2}}{\sum_{i=1}^{n}x_{i}^2}
\end{aligned}
$$
On déduit que: $Var(\tilde{\beta})$ est strictement plus grande que  $Var(\hat{\beta})$


Dans le cas où tous les $x_{i}$ sont égaux; par exemple à x , on a :
$$

\sum_{i=1}^{n} x_{i}^2=nx \ \text{et} \ (\sum_{i=1}^{n} x_{i})^2= n^2x^2

$$
Donc:

$$
Var(\tilde{\beta})= \frac{n\sigma^{2}}{(\sum_{i=1}^{n}x_{i}^2)^2} =\frac{n\sigma^2}{n^2x^2}=\frac{\sigma^2}{nx^2}
$$
Ainsi: 
$$
Var(\hat{\beta})=\frac{\sigma^{2}}{\sum_{i=1}^{n}x_{i}^2}=\frac{\sigma^2}{nx^2}
$$
D'où: $Var(\hat{\beta})=Var(\tilde{\beta})$. 

On en déduit donc que sous l'hypothèse d'égalité des $x_{i}$; on a $Var(\hat{\beta})=Var(\tilde{\beta})$