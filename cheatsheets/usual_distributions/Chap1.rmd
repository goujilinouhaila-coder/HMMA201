---
title: "chap1"
author: "HMMA201"
date: "30/01/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Introduction:


La régression linéaire simple permet de montrer l'éventuelle relation fonctionnelle linéaire qui existerait entre une variable **EXPLICATIVE** (ou indépendante) x et une variable aléatoire à EXPLIQUER (ou dépendante) y. En revanche, pour analyser la relation entre les $x_{i}$ et les $y_{i}$; on cherche une fonction f telle que

 $$
 y_{i} \approx f(x_{i})
 $$ 
##La droite de régression:
 
 Nous observons graphiquement un nuage de points représentant les observations. Celui-ci est de forme plus au moins rectiligne. La question qu'on a tendance à poser: Comment trouver l'équation de la droite qui le résume au mieux? Théoriquement; il faut minimiser les distances qui la séparent des points. 

  Le problème mathématique dont on va s'intéresser à modéliser: peut s'écrire:

$$
 argmin_{f\in F} \sum_{i=1}^n L(y_{i}-f(x_{i}))

$$
Avec:

F= L'ensemble des fonctions affinne de R dans R
 
n=la taille de l'échantillon
 
L(.) est la fonction de perte (coût) où on va priviligier celle de coût quadratique pour la méthode des Moindres Carrées Ordinaires(MCO).

##Principe de régression linéaire simple:

Chercher f dans l'ensemble F des fonctions affines de R dans R

###Modèle de régression linéaire simple:

$$
\forall{i \in (1,....,n)}; y_{i}=\beta_{1}+\beta_{2}*x_{i}+\epsilon_{i}; \\\\\\\\\\\\\\\\\\

 \epsilon_{i}= \text{les erreurs}. 

$$

##Hypothèses du modèle:

$$ 
(H): \left\{
              \begin{array}{ll}
              (H_{1}): E(\epsilon_{i})=0 & \forall i ; \\\
               (H_{2}): cov(\epsilon_{i},\epsilon_{j})=\delta_{ij}*\sigma^{2} & \forall (i,j)\\
              \end{array}
        \right.      
$$
##Notation vectorielle:

La notaton vectorielle est similaire à la notation simple. On définit: 

$$

Y=[y_{1},....,y_{n}]^{'} \text{est aléatoire de dimension n.}\\\\\
    
    
X=[x_{1},.....x_{n}]^{'} \text{est non aléatoire de dimension n.}\\\ 
    
    
\beta_{1} \text{et} \beta_{2}\ \text{sont des coefficients non aléatoire}. \\\\
    
    
\epsilon=[\epsilon_{1},.....\epsilon_{n}]^{'} \text{est aléatoire de dimension n.} \\
$$

    
Le modèle se réecrit donc de la façon suivante: 

$$
Y=\beta_{1}*\mathbb{1}+\beta_{2}*X+\epsilon
$$ 
##Moindres Carrées Ordinaires(MCO):

###Estimateurs des Moindres Carrées Ordinaires: 

Soit la quantité:

$$
S(\beta_{1},\beta_{2})= \sum_{i=1}^{n}(y_{i}-\beta_{1}-\beta_{2}*x_{i})^2
$$
Les estimateurs de MCO: 
$\hat{beta_{1}}$ et $\hat{\beta_{2}}$ minimisent $S(\beta_{1},\beta_{2})$ ont pour expression:

$$
\hat{\beta_{1}}=\bar{y}-\hat{\beta_{2}}*\bar{x} \\
\text{Avec:} \ \hat{\beta_{2}}=\frac{\sum_{i=1}^n(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^n(x_{i}-\bar{x})^2}
$$


Cette expression de l'estimateur $\hat{\beta_{1}}$ montre que la droite des MCO par le centre de gravité 

du nuage ($\bar{x}$,$\bar{y}$). \\

Dans le cas, où les termes d'erreurs sont indépendants et identiquement distribués;l'estimateur des moindres carrés ordinaires est le plus efficace des estimateurs linéaires sans biais.  

###Propriétés des estimateurs $\hat{\beta_{1}}$ et $\hat{\beta_{2}}$: 

 * $\hat{\beta_{1}}$ et $\hat{\beta_{2}}$ sont des estimateurs sans biais de $\beta_{1}$ et $\beta_{2}$. \\
 
 * Parmi les estimateurs sans biais linéaires en y, les estimateurs $\hat{\beta_{j}}$ sont de **variances minimales**.\\\
 
 **Les variances des estimateurs:** \\
 
$$
*\ Var(\hat{\beta_{1}})= \frac{\sigma^{2}\sum_{i=1}^{n}x_{i}^{2}}{n\sum _{i=1}^{n}(x_{i}-\bar{x})^2}= \sigma^{2}(\frac{1}{n}+\frac{\bar{x}^2}{n\sum _{i=1}^{n}(x_{i}-\bar{x})^2}) \\\\\\\\

* \ Var(\hat{\beta_{2}})= \frac{\sigma^{2}}{\sum _{i=1}^{n}(x_{i}-\bar{x})^2}
$$ 
**Leur covariance**: \\ 

$$
cov(\hat{\beta_{1}},\hat{\beta_{2}})= -\frac{\sigma^{2}\bar{x}^{2}}{\sum _{i=1}^{n}(x_{i}-\bar{x})^2}

$$
##Calcul des résidus et de la variance résiduelle: 

$\hat{\beta_{1}}$ est l'ordonnée à l'origine et $\hat{\beta_{2}}$ la pente de la droite. \\\
Les résidus sont définis par: $\hat{\epsilon_{i}}=y_{i}-\hat{y_{i}}=y_{i}-\hat{\beta_{1}}-\hat{\beta_{2}}x_{i}=(y_{i}-\bar{y})-\hat{\beta_{2}}(x_{i}-\bar{x})$.\\\

Par construction: $\sum_{i=1}^{n} \hat{\epsilon_{i}}= \sum_{i=1}^{n} (y_{i}-\bar{y})-\hat{\beta_{2}}\sum_{i=1}^{n}(x_{i}-\bar{x})=0$ \\

##Estimateur non biaisé de$ $\sigma^{2}$: \\ 

$$
\hat{\sigma}^{2}=\sum_{i=1}^{n} \frac{\hat{\epsilon_{i}}^{2}}{n-2} \\
\text{est un estimateur SB de}\ \sigma^{2}
$$
Lorsque n est grand, $\hat{\sigma}^{2}$ diffère très peu de **l'estimateur empirique de la variance des résidus**:\\ $\sum_{i=1}^{n} \frac{\hat{\epsilon_{i}}^{2}}{n}$


###Remarques:

* La variance de $\hat{\beta_{1}}$ est plus petite lorsque les $x_{i}$ sont plus étalés. \\\

* La variance de $\hat{\beta_{2}}$ est indéterminé lorsques $x_{i}$ sont plus étalés.\\\

##Prévision: 

Le modèle est toujours le même: $y_{n+1}=\beta_{1}+\beta_{2}x_{n+1}+\epsilon{n+1}$ avec $\mathbb{E}(\epsilon_{n+1})=0$, $Var(\epsilon_{n+1})=\sigma^{2}$ et $cov(\epsilon_{n+1},\epsilon_{i})=0$ $\forall i=1,....,n$. \\

$$
\hat{y}_{n+1}=\hat{\beta_{1}}+\hat{\beta_{2}}x_{n+1}
$$

###Erreur de Prévision: \\\ 
$$

\left\{
       \begin{array}{ll}
        \mathbb{E}(\hat{\epsilon}_{n+1})=0 \\\
        Var(\hat{\epsilon}_{n+1})= \sigma^{2}(1+\frac{1}{n}+\frac{(x_{n+1}-\bar{x})^{2})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2})
        
        \end{array} 
\right.


$$

##Interprétation géométrique: \\

Il s'agit de trouver le meilleur modèle qui permet de prédire y à l'aide d'une combinaison linéaire des variables de X. \\

On cherche donc les coefficients $\hat{\beta_{1}}$ et $\hat{\beta_{2}}$ tel que 

$$
\tilde{y}=X \tilde{\beta_{2}}+\tilde{\beta_{1}} \mathbb{1}
$$ 
minimisant $\|Y-\tilde{Y}\|$ qui revient à minimiser son carré: \\

$$
\|Y-\tilde{Y}\|^2= \sum_{i=1}^n(y_{i}-(\tilde{\beta_{1}}+\tilde{\beta_{2}}x_{i}))^2

$$
**Conclusion:** 

L'estimation de Y est la projection de Y sur l'espace engendré par les colonnes de X et par $\mathbb{1}$. c-à-d par l'ensemble des combinaisons linéaires possibles des colonnes de X et de $\mathbb_{1}$. \\\

